{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T02:36:57.273918Z",
     "start_time": "2024-06-03T02:36:55.469977Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "from ngram import generate_ngram_models\n",
    "from nltk.corpus import reuters\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 读取字典库、语料库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T02:36:57.319797Z",
     "start_time": "2024-06-03T02:36:57.277907Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab = {line.rstrip() for line in open('vocab.txt')}\n",
    "#用set来存储不用list是因为查找的时候set时间复杂度是O(1),List是O(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T02:36:57.900651Z",
     "start_time": "2024-06-03T02:36:57.322801Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to\n",
      "[nltk_data]     C:\\Users\\coldwarrior\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\coldwarrior\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('reuters')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 据语料库生成ngram模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T02:37:07.621829Z",
     "start_time": "2024-06-03T02:36:57.902663Z"
    }
   },
   "outputs": [],
   "source": [
    "categories = reuters.categories()  # 路透社语料库的类别\n",
    "corpus = reuters.sents(categories=categories)  # sents()指定分类中的句子\n",
    "\n",
    "# 构建语言模型：bigram\n",
    "term_count, bigram_count=generate_ngram_models(corpus,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 从count_1edit.txt获取channel probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T02:37:07.667051Z",
     "start_time": "2024-06-03T02:37:07.624822Z"
    }
   },
   "outputs": [],
   "source": [
    "# 用户打错的概率统计 - channel probability\n",
    "# 创建一个字典来存储channel probabilities\n",
    "channel_prob = {}\n",
    "total_errors = 0\n",
    "\n",
    "i = 0\n",
    "# 解析错误数据并计算总错误次数\n",
    "for line in open('count_1edit.txt'):\n",
    "    i += 1\n",
    "    # Step1:解析数据\n",
    "    # 正则表达式找到错误次数\n",
    "    count = re.findall(r'\\d+', line)[-1]\n",
    "    \n",
    "    # 从末尾剥离数字\n",
    "    line = line.replace(count, \"\")\n",
    "    # 剥离制表符\n",
    "    if \"\\t\" in line:\n",
    "        line = line.replace(\"\\t\", \"\")\n",
    "    # 判断空格在不在后段\n",
    "    first, last = line.split(\"|\")\n",
    "\n",
    "    if \" \" in last:\n",
    "        # 去除多个空格为一个    \n",
    "        if re.match(r\" {2,}\", line):\n",
    "            multi_spaces = re.findall(r\" {2,}\", line)\n",
    "            for space in multi_spaces:\n",
    "                line = line.replace(space, \" \")\n",
    "    \n",
    "    # 正常情况\n",
    "    correct, mistake = line.split(\"|\")\n",
    "\n",
    "    count = int(count)\n",
    "    # Step2:计算错误次数\n",
    "    if correct not in channel_prob:\n",
    "        channel_prob[correct] = {}\n",
    "\n",
    "    channel_prob[correct][mistake] = count\n",
    "    total_errors += count\n",
    "\n",
    "# 计算每种错误的概率\n",
    "for correct in channel_prob:\n",
    "    for mistake in channel_prob[correct]:\n",
    "        channel_prob[correct][mistake] /= total_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建生成者\n",
    "from edit_distance import CandidatesGenerator\n",
    "CG = CandidatesGenerator(vocab=vocab)\n",
    "# 设置最大编辑距离\n",
    "max_distance = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T02:40:00.668320Z",
     "start_time": "2024-06-03T02:39:40.334251Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing lines: 100%|██████████| 1000/1000 [00:00<00:00, 3688.27it/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "def count_lines(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        return sum(1 for line in f)\n",
    "\n",
    "# 计算文件的行数\n",
    "file_path = \"testdata.txt\"\n",
    "total_lines = count_lines(file_path)\n",
    "V = len(term_count.keys())\n",
    "# 打开文件\n",
    "with open(file_path, \"r\") as file:\n",
    "    results = []\n",
    "    i = 1\n",
    "    bar = tqdm(file, total=total_lines, desc=\"Processing lines\")\n",
    "    \n",
    "    for line in bar:\n",
    "        line = re.sub(r\"([,])([^\\d])\", r\" \\1 \\2\", line)\n",
    "        \n",
    "        line = re.sub(r\"([^s])(['])\", r\"\\1 \\2\", line)\n",
    "        line = re.sub(r\"([s])(['])\", r\"\\1 \\2 \", line)\n",
    "\n",
    "        line = re.sub(r\"([.]$)\", r\" \\1 \", line)\n",
    "        # items = line.rstrip().split(\"\\t\")\n",
    "        items = line.split(\"\\t\")\n",
    "        line = items[2].split()\n",
    "        corrected_line = line\n",
    "        j = 0\n",
    "        \n",
    "        for word in line:\n",
    "            if word not in vocab:\n",
    "                # 需要替换word成正确的单词\n",
    "                # Step1: 生成所有的(valid)候选集合\n",
    "                # 获得编辑距离小于2的候选列表\n",
    "                # candidates1 = generate_candidates(word, vocab, 1)\n",
    "                # candidates2 = generate_candidates(word, vocab, 2)\n",
    "                # candidates = list(candidates1.union(candidates2))\n",
    "                candidates = CG.generate_candidates(word,max_distance=max_distance)\n",
    "                candidates = list(candidates)\n",
    "                probs = []\n",
    "                \n",
    "                # 对于每一个candidate, 计算它的score\n",
    "                # score = p(correct)*p(mistake|correct)\n",
    "                #       = log p(correct) + log p(mistake|correct)\n",
    "                # 返回score最大的candidate\n",
    "                for candi in candidates:\n",
    "                    prob = 0\n",
    "                    # 计算channel probability\n",
    "                    if candi in channel_prob and word in channel_prob[candi]:\n",
    "                        prob += np.log(channel_prob[candi][word])\n",
    "                    else:\n",
    "                        prob += np.log(0.0001)\n",
    "                    \n",
    "                    # 计算语言模型的概率\n",
    "                    \"\"\"\n",
    "                   比如s=I like playing football.\n",
    "                   line=['I','like','playing','football']\n",
    "                   word为playing时\n",
    "                   \"\"\"\n",
    "                    forward_word = (\n",
    "                        line[j - 1] + \" \" + candi\n",
    "                    )  # 考虑前一个单词,出现like playing的概率\n",
    "\n",
    "                    if forward_word in bigram_count and line[j - 1] in term_count:\n",
    "                        prob += np.log(\n",
    "                            (bigram_count[forward_word] + 1.0)\n",
    "                            / (term_count[line[j - 1]] + V)\n",
    "                        )\n",
    "                    else:\n",
    "                        prob += np.log(1.0 / V)\n",
    "\n",
    "                    if j + 1 < len(line):  # 考虑后一个单词，出现playing football的概率\n",
    "                        backward_word = candi + \" \" + line[j + 1]\n",
    "                        if backward_word in bigram_count and candi in term_count:\n",
    "                            prob += np.log(\n",
    "                                (bigram_count[backward_word] + 1.0)\n",
    "                                / (term_count[candi] + V)\n",
    "                            )\n",
    "                        else:\n",
    "                            prob += np.log(1.0 / V)\n",
    "                    probs.append(prob)\n",
    "\n",
    "                if probs:\n",
    "                    max_idx = probs.index(max(probs))\n",
    "                    if len(word) == 1:\n",
    "                        corrected_line[j] = word  # 不替换单个字母\n",
    "                    else:\n",
    "                        corrected_line[j] = candidates[max_idx]\n",
    "            j += 1\n",
    "\n",
    "        corrected_sentence = \" \".join(corrected_line)\n",
    "        corrected_sentence = re.sub(r\"\\s*(['])\\s*\", r\"\\1\", corrected_sentence)  # 去除标点前的空格\n",
    "        corrected_sentence = re.sub(r\"(s')\", r\"\\1 \", corrected_sentence)  # 恢复s'的情况\n",
    "\n",
    "\n",
    "        corrected_sentence = re.sub(r\"\\s([.])\\s\", r\"\\1\", corrected_sentence)  # 去除标点前的空格\n",
    "\n",
    "        corrected_sentence = re.sub(r\"\\s([,])\", r\"\\1\", corrected_sentence)  # 去除标点前的空格（保留逗号后面的空格）\n",
    "        corrected_sentence = re.sub(r\"(\\d)([,])\\s+(\\d)\", r\"\\1\\2\\3\", corrected_sentence)  # 去除数据中的空格\n",
    "        # 句点补全\n",
    "        if corrected_sentence[-1] != \".\":\n",
    "            corrected_sentence += \".\"\n",
    "        \n",
    "        results.append(f\"{i}\\t{corrected_sentence}\")\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-06-03T02:37:16.110450Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(\"result.txt\", \"w\") as file:\n",
    "    file.write(\"\\n\".join(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 直接调用测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is : 89.80%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "count = os.system(\"python eval_enhanced.py\")\n",
    "print(\"Accuracy is : %.2f%%\" % (count * 1.00 / 10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
